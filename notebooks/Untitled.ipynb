{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../fairml')\n",
    "import plot\n",
    "import generate\n",
    "import models\n",
    "\n",
    "ctr=1\n",
    "generate_toys = generate.toys_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, name, depth, width):\n",
    "\n",
    "        self.depth = depth\n",
    "        self.width = width\n",
    "        self.name = name\n",
    "\n",
    "\n",
    "class Classifier(Model):\n",
    "\n",
    "    def __init__(self, name, depth=3, width=10, n_classes=2):\n",
    "\n",
    "        super().__init__(name, depth, width)\n",
    "        self.n_classes = 2\n",
    "\n",
    "    def build_forward(self, x_in):\n",
    "\n",
    "        with tf.variable_scope(self.name):\n",
    "\n",
    "            # input layer\n",
    "            layer = x_in\n",
    "\n",
    "            # hidden layers\n",
    "            for _ in range(self.depth):\n",
    "                layer = layers.relu(layer, self.width)\n",
    "\n",
    "            # logits and output\n",
    "            self.logits = layers.linear(layer, self.n_classes)\n",
    "            self.output = tf.reshape(layers.softmax(self.logits)[:, 1], shape=(-1, 1))\n",
    "\n",
    "        self.vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "\n",
    "    def build_loss(self, labels):\n",
    "\n",
    "        # one hot encode the labels\n",
    "        one_hot = tf.one_hot(tf.reshape(labels, shape=[-1]), depth=self.n_classes)\n",
    "\n",
    "        # and build the loss\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=one_hot, logits=self.logits))\n",
    "        \n",
    "\n",
    "class Adversary(Model):\n",
    "    \n",
    "    def __init__(self, name, depth=1, width=5):\n",
    "        \n",
    "        super().__init__(name, depth, width)\n",
    "        self.loss = None\n",
    "        self.vars = None\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, adv_settings):\n",
    "        \n",
    "        adv_type = adv_settings['adv_type']\n",
    "        \n",
    "        classes = {'Dummy':DummyAdversary,\n",
    "                   'GMM':GMMAdversary}\n",
    "        \n",
    "        # check if implemented\n",
    "        if adv_type not in classes:\n",
    "            raise ValueError('Unknown Adversary type {}.'.format(adv_type))\n",
    "        \n",
    "        # return the right one\n",
    "        adversary = classes[adv_type]\n",
    "        adv_settings.pop('adv_type')\n",
    "        return adversary(name=adv_type, **adv_settings)\n",
    "\n",
    "        \n",
    "class GMMAdversary(Adversary):\n",
    "    \n",
    "    def __init__(self, name, **kwargs):\n",
    "        \n",
    "        super().__init__(name, **kwargs)\n",
    "\n",
    "class DummyAdversary(Adversary):\n",
    "    \n",
    "    def __init__(self, name, **kwargs):\n",
    "        \n",
    "        super().__init__(name, **kwargs)\n",
    "        \n",
    "    def build_loss(self, fX, Z):\n",
    "        \n",
    "        with tf.variable_scope(self.name):\n",
    "            dummy_var = tf.Variable(0.001, name='dummy')\n",
    "            self.loss = dummy_var**2 # i.e. goes to zero\n",
    "\n",
    "        self.vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# environment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class TFEnvironment:\n",
    "    \n",
    "    def __init__(self, generate, name):\n",
    "        \n",
    "        # save variables\n",
    "        self.generate = generate\n",
    "        self.name = name\n",
    "        self._xyz = ['X', 'Y', 'Z']\n",
    "        \n",
    "        # record losses\n",
    "        self.loss_history = {'clf':[], 'adv':[], 'comb':[]}\n",
    "        \n",
    "        # start the session\n",
    "        self._start_session()\n",
    "    \n",
    "    def _start_session(self):\n",
    "        \n",
    "        # start the session\n",
    "        config=tf.ConfigProto(intra_op_parallelism_threads = 32,\n",
    "                                 inter_op_parallelism_threads = 32,\n",
    "                                 allow_soft_placement = True,\n",
    "                                 device_count = {'CPU': 2})\n",
    "        \n",
    "        self.sess = tf.Session(config=config)\n",
    "    \n",
    "    def build_graph(self, clf_settings, adv_settings):\n",
    "        \n",
    "        print('--- Building computational graphs')\n",
    "        \n",
    "        # build the inputs\n",
    "        batch = self.generate(10)\n",
    "        self._in = {}\n",
    "        for xyz in self._xyz:\n",
    "            tftype = tf.int32 if xyz == 'Y' else tf.float32\n",
    "            self._in[xyz] = tf.placeholder(tftype, shape=(None, batch[xyz].shape[1]), name='{}_in'.format(xyz))\n",
    "        \n",
    "        # build the classifier graph\n",
    "        self.clf = Classifier(name=self.name+'_clf', **clf_settings)\n",
    "        self.clf.build_forward(self._in['X'])\n",
    "        \n",
    "        # build the adversary graph\n",
    "        self.adv = Adversary.create(adv_settings)\n",
    "        \n",
    "    def build_loss(self):\n",
    "        \n",
    "        print('--- Building computational graphs for losses')\n",
    "        \n",
    "        # classifier loss\n",
    "        self.clf.build_loss(self._in['Y'])\n",
    "        \n",
    "        # adversary loss\n",
    "        self.adv.build_loss(self.clf.output, self._in['Z'])\n",
    "        \n",
    "    def build_opt(self, lam=1, opt_type='AdamOptimizer', learning_rate=0.05, projection=False):\n",
    "        \n",
    "        print('--- Building computational graphs for optimisations')\n",
    "        \n",
    "        # optimizer type\n",
    "        opt = getattr(tf.train, opt_type)\n",
    "        self.optimizer = opt(learning_rate=learning_rate)\n",
    "        self.lam = lam\n",
    "        \n",
    "        # compute the gradients\n",
    "        self.grad_clf = self.optimizer.compute_gradients(self.clf.loss, var_list=self.clf.vars)\n",
    "        self.grad_adv = self.optimizer.compute_gradients(self.adv.loss, var_list=self.adv.vars)\n",
    "        self.grad_comb = []\n",
    "        \n",
    "        # compute the optimisation steps\n",
    "        self.opt_clf = self.optimizer.apply_gradients(self.grad_clf)\n",
    "        self.opt_adv = self.optimizer.apply_gradients(self.grad_adv)\n",
    "        self.opt_comb = None #self.optimizer.apply_gradients(self.grad_comb)\n",
    "        \n",
    "    def initialise_variables(self):\n",
    "\n",
    "        print('--- Initialising TensorFlow variables')\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _get_feed_dict(self, batch):\n",
    "        \n",
    "        return {self._in[xyz]:batch[xyz] for xyz in self._xyz}\n",
    "    \n",
    "    def pretrain_step(self, batch_size):\n",
    "        \n",
    "        # pre-training step (no adversary)\n",
    "        batch = self.generate(batch_size)\n",
    "        feed_dict = self._get_feed_dict(batch)\n",
    "        self.sess.run(self.opt_clf, feed_dict=feed_dict)\n",
    "        \n",
    "        # update loss history\n",
    "        self._write_loss_history(batch)\n",
    "        \n",
    "    def train_step_clf(self, batch_size):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def train_step_adv(self, batch_size):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def _write_loss_history(self, batch):\n",
    "        \n",
    "        # get current losses\n",
    "        loss_clf, loss_adv, loss_comb = self._losses(batch)\n",
    "        \n",
    "        # append\n",
    "        self.loss_history['clf'].append(loss_clf)\n",
    "        self.loss_history['adv'].append(loss_adv)\n",
    "        self.loss_history['comb'].append(loss_comb)\n",
    "    \n",
    "    def _losses(self, batch):\n",
    "        \n",
    "        feed_dict = self._get_feed_dict(batch)\n",
    "        loss_clf, loss_adv = self.sess.run([self.clf.loss, self.adv.loss], feed_dict=feed_dict)\n",
    "        return loss_clf, loss_adv, loss_clf - self.lam * loss_adv\n",
    "        \n",
    "    def predict(self, batch):\n",
    "        \n",
    "        feed_dict = self._get_feed_dict(batch)\n",
    "        preds = self.sess.run(self.clf.output, feed_dict=feed_dict)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def show_variates(self, batch_size):\n",
    "        \n",
    "        print('--- Plot random variates')\n",
    "        \n",
    "        # prepare the plot\n",
    "        batch = self.generate(batch_size)\n",
    "        fig, ax = plt.subplots(2, 2,\n",
    "                               figsize=(7,7),\n",
    "                               gridspec_kw={'height_ratios':[1,4],\n",
    "                                            'width_ratios':[4,1]},\n",
    "                               sharex='col',\n",
    "                               sharey='row')\n",
    "\n",
    "        # main plot\n",
    "        plot.variates_main(ax[1,0], batch)\n",
    "        \n",
    "        # top plot\n",
    "        plot.variates_kde(ax[0,0], batch, x_cpt=0)\n",
    "        \n",
    "        # right plot\n",
    "        plot.variates_kde(ax[1,1], batch, x_cpt=1)\n",
    "    \n",
    "        # empty axes\n",
    "        fig.delaxes(ax[0,1])\n",
    "        fig.show()\n",
    "    \n",
    "    def show_performance(self, batch_size):\n",
    "        \n",
    "        print('--- Plot classifier performance and fairness')\n",
    "        \n",
    "        # get predictions\n",
    "        batch = {}\n",
    "        preds = {}\n",
    "        for z in [None, 1, 0, -1]:\n",
    "            batch[z] = self.generate(batch_size, z=z)\n",
    "            preds[z] = self.predict(batch[z])\n",
    "        \n",
    "        # prepare the figure\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "        \n",
    "        # plot the variates\n",
    "        plot.variates_main(ax[0,0], batch[None])\n",
    "        \n",
    "        # plot the ROC curves\n",
    "        plot.roc_curves(ax[1,0], batch[1], batch[0], batch[-1], preds[1], preds[0], preds[-1])\n",
    "        \n",
    "        # plot the classifier output\n",
    "        plot.clf_outputs(ax[1,1], preds[1], preds[0], preds[-1])\n",
    "        \n",
    "        # plot the decision boundary\n",
    "        plot.decision_boundary(ax[0,1], batch[None], preds[None])\n",
    "        \n",
    "        # show\n",
    "        fig.show()\n",
    "        \n",
    "    def show_loss_history(self):\n",
    "        \n",
    "        print('implement me!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "ctr+=1\n",
    "\n",
    "clf_settings = {'depth':3, 'width':10}\n",
    "adv_settings = {'adv_type':'Dummy'}\n",
    "opt_settings = {'lam':50, 'opt_type':'AdamOptimizer', 'learning_rate':0.05}\n",
    "\n",
    "# make the environment\n",
    "tfe = TFEnvironment(generate_toys, 'tf_env_{}'.format(ctr))\n",
    "tfe.build_graph(clf_settings, adv_settings)\n",
    "tfe.build_loss()\n",
    "tfe.build_opt(**opt_settings)\n",
    "tfe.initialise_variables()\n",
    "\n",
    "for i in range(50):\n",
    "    tfe.pretrain_step(10)\n",
    "#tfe.show_variates(4000)\n",
    "tfe.show_performance(4000)\n",
    "tfe.show_loss_history()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
