{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../fairml')\n",
    "import plot\n",
    "import generate\n",
    "import models\n",
    "import environment\n",
    "\n",
    "\n",
    "import itertools\n",
    "from importlib import reload\n",
    "reload(generate)\n",
    "reload(plot)\n",
    "\n",
    "\n",
    "generate_toys = generate.toys_simple\n",
    "#generate_toys = generate.toys_expo_Z\n",
    "#generate_toys = generate.toys_discrete_Z\n",
    "#generate_toys = generate.toys_peaked_Z\n",
    "\n",
    "plot.show_variates(generate_toys, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a single environment as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "reload(generate)\n",
    "reload(plot)\n",
    "reload(models)\n",
    "reload(environment)\n",
    "\n",
    "clf_settings = {'depth':2, 'width':20}\n",
    "adv_settings = {'adv_type':'GMM'}\n",
    "opt_settings = {'lam':1, 'opt_type':'AdamOptimizer', 'learning_rate':0.01, 'projection':False}\n",
    "trn_settings = {'n_pretrain':30, 'batch_size':1000, 'n_train':100}\n",
    "\n",
    "# make the environment\n",
    "tfe = environment.TFEnvironment(generate_toys, 'env_{}'.format(uuid.uuid4()))\n",
    "tfe.build_graph(clf_settings, adv_settings)\n",
    "tfe.build_loss()\n",
    "tfe.build_opt(**opt_settings)\n",
    "tfe.initialise_variables()\n",
    "\n",
    "# pretrain\n",
    "batch_size = trn_settings['batch_size']\n",
    "n_pretrain = trn_settings['n_pretrain']\n",
    "for _ in range(n_pretrain):\n",
    "    tfe.pretrain_step_clf(batch_size, write=False)\n",
    "for _ in range(n_pretrain):\n",
    "    tfe.train_step_adv(batch_size, write=False)\n",
    "\n",
    "# train\n",
    "n_train = trn_settings['n_train']\n",
    "for _ in range(n_train):\n",
    "\n",
    "    tfe.train_step_clf(batch_size)\n",
    "    for __ in range(5):\n",
    "        tfe.train_step_adv(batch_size)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload(plot)\n",
    "tfe.show_performance(4000)\n",
    "tfe.show_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train multiple environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from importlib import reload\n",
    "reload(environment)\n",
    "reload(models)\n",
    "\n",
    "# setting signature\n",
    "clf_settings = {'depth':2, 'width':20}\n",
    "adv_settings = {'adv_type':'MINE'}\n",
    "opt_settings = {'lam':1, 'opt_type':'AdamOptimizer', 'learning_rate':0.01, 'projection':False}\n",
    "trn_settings = {'n_pretrain':30, 'batch_size':1000, 'n_train':100}\n",
    "\n",
    "# prepare the loop\n",
    "adversaries = ['MINE', 'GMM', 'PtEst']\n",
    "projections = [True, False]\n",
    "optimizers = ['AdamOptimizer']\n",
    "\n",
    "# prepare the results\n",
    "envs = {adv:{proj:{opt:[] for opt in optimizers} for proj in projections} for adv in adversaries}\n",
    "\n",
    "# run the loop\n",
    "for adv, proj, opt in itertools.product(adversaries, projections, optimizers):\n",
    "    \n",
    "    print(adv, proj, opt)\n",
    "\n",
    "    adv_settings['adv_type'] = adv\n",
    "    opt_settings['projection'] = proj\n",
    "    opt_settings['opt_type'] = opt\n",
    "    envs[adv][proj][opt] = environment.bootcamp(10, generate_toys, clf_settings, adv_settings, opt_settings, trn_settings)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from importlib import reload\n",
    "reload(plot)\n",
    "\n",
    "fig, ax = plt.subplots(6, figsize=(10,15))\n",
    "fig.suptitle('Performance for different adversaries (lambda={})'.format(opt_settings['lam']))\n",
    "colors ={'MINE':'navy', 'GMM':'royalblue', 'PtEst':'tomato'}\n",
    "for adv, opt in itertools.product(adversaries, optimizers):\n",
    "    \n",
    "    # classifier loss\n",
    "    plot.history(ax[0], [e.history['L_clf'] for e in envs[adv][False][opt]], color=colors[adv], label='Classifier loss ({})'.format(adv))\n",
    "\n",
    "    # adversary loss\n",
    "    plot.history(ax[1], [e.history['L_adv'] for e in envs[adv][False][opt]], color=colors[adv], label='Adversary loss ({})'.format(adv))\n",
    "                 \n",
    "    # combined loss\n",
    "    plot.history(ax[2], [e.history['L_comb'] for e in envs[adv][False][opt]], color=colors[adv], label='Combined loss ({})'.format(adv))\n",
    "    \n",
    "    # AUROC\n",
    "    plot.history(ax[3], [e.history['auroc-mean'] for e in envs[adv][False][opt]], color=colors[adv], label='mean(AUROC), Z=(1,0,-1) ({})'.format(adv))\n",
    "    plot.history(ax[4], [e.history['auroc-std'] for e in envs[adv][False][opt]], color=colors[adv], label='std(AUROC), Z=(1,0,-1) ({})'.format(adv))\n",
    "    \n",
    "    # KS\n",
    "    plot.history(ax[5], [e.history['KS'] for e in envs[adv][False][opt]], color=colors[adv], label='KS metric ({})'.format(adv))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    ax[i].legend(loc='best')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for adv, opt in itertools.product(adversaries, optimizers):\n",
    "    \n",
    "    # create the figure\n",
    "    fig, ax = plt.subplots(6, figsize=(10,15))\n",
    "    fig.suptitle('Performance for {} with {} (lambda={})'.format(adv, opt, opt_settings['lam']))\n",
    "    \n",
    "    # classifier loss\n",
    "    plot.history(ax[0], [e.history['L_clf'] for e in envs[adv][True][opt]], color='crimson', label='Classifier loss (with proj)')\n",
    "    plot.history(ax[0], [e.history['L_clf'] for e in envs[adv][False][opt]], color='olive', label='Classifier loss (w/o proj)')\n",
    "\n",
    "    # adversary loss\n",
    "    plot.history(ax[1], [e.history['L_adv'] for e in envs[adv][True][opt]], color='crimson', label='Adversary loss (with proj)')\n",
    "    plot.history(ax[1], [e.history['L_adv'] for e in envs[adv][False][opt]], color='olive', label='Adversary loss (w/o proj)')\n",
    "\n",
    "    # combined loss\n",
    "    plot.history(ax[2], [e.history['L_comb'] for e in envs[adv][True][opt]], color='crimson', label='Combined loss (with proj)')\n",
    "    plot.history(ax[2], [e.history['L_comb'] for e in envs[adv][False][opt]], color='olive', label='Combined loss (w/o proj)')\n",
    "\n",
    "    # AUROC\n",
    "    plot.history(ax[3], [e.history['auroc-mean'] for e in envs[adv][True][opt]], color='crimson', label='mean(AUROC) (with proj)')\n",
    "    plot.history(ax[3], [e.history['auroc-mean'] for e in envs[adv][False][opt]], color='olive', label='mean(AUROC) (w/o proj)')\n",
    "    \n",
    "    plot.history(ax[4], [e.history['auroc-std'] for e in envs[adv][True][opt]], color='crimson', label='std(AUROC) (with proj)')\n",
    "    plot.history(ax[4], [e.history['auroc-std'] for e in envs[adv][False][opt]], color='olive', label='std(AUROC) (w/o proj)')\n",
    "    \n",
    "    # KS\n",
    "    plot.history(ax[5], [e.history['KS'] for e in envs[adv][True][opt]], color='crimson', label='KS metric (with proj)')\n",
    "    plot.history(ax[5], [e.history['KS'] for e in envs[adv][False][opt]], color='olive', label='KS metric (w/o proj)')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for i in range(6):\n",
    "        ax[i].legend(loc='best')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (FairML)",
   "language": "python",
   "name": "fairml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
